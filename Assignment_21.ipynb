{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd3ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance\n",
    "training set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe42355",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. The estimated depth of a Decision Tree trained on a one million instance training set can vary depending on the complexity and patterns in the data. In an unrestricted setting, the depth can potentially be very deep, reaching the number of instances in the training set. However, the actual depth will depend on the features, the target variable, and the interactions between them. It is not possible to determine the exact depth without considering the specific characteristics of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always\n",
    "lower/greater, or is it usually lower/greater?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af147c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. The Gini impurity of a node is generally lower than that of its parent. The Gini impurity measures the impurity or disorder of a node, and the goal of the Decision Tree algorithm is to reduce impurity as much as possible at each step. By splitting the data into child nodes, the impurity is typically reduced, leading to lower Gini impurity values in the child nodes compared to their parent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adad672",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68264767",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. It can be a good idea to reduce the maximum depth of a Decision Tree if it is overfitting the training set. Overfitting occurs when the tree becomes too deep and captures noise or outliers in the training data, resulting in poor generalization to unseen data. By reducing the maximum depth, the tree is constrained to capture fewer intricate details of the training data, which can help prevent overfitting and improve generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training\n",
    "set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9905b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Scaling the input features is generally not necessary for Decision Trees because they are not affected by the scale of the features. Decision Trees make splits based on feature thresholds, and the scale of the features does not impact the relative ordering of the values within a feature. However, if a Decision Tree underfits the training set, scaling the features is unlikely to have a significant impact on its performance. Underfitting usually occurs when the tree is too shallow or lacks complexity to capture the patterns in the data. In such cases, increasing the maximum depth or using ensemble methods like Random Forests may be more effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "5. How much time will it take to train another Decision Tree on a training set of 10 million instances\n",
    "if it takes an hour to train a Decision Tree on a training set with 1 million instances?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dfa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. The time it takes to train another Decision Tree on a training set of 10 million instances cannot be accurately determined solely based on the time taken to train a Decision Tree on a training set with 1 million instances. The time required to train a Decision Tree depends on various factors, including the complexity of the data, the implementation of the algorithm, hardware resources, and other parameters. Generally, training time tends to increase with the size of the training set, so training a Decision Tree on a larger dataset will likely take longer than training on a smaller dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "6. Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "6. Setting `presort=True` is unlikely to speed up training if the training set has 100,000 instances. The `presort` option in Decision Tree algorithms pre-sorts the data based on feature values, which can be time-consuming for large datasets. It is generally recommended to enable `presort=True` only when the dataset is relatively small, typically with fewer than 10,000 instances. For larger datasets, the overhead of pre-sorting may outweigh any potential benefits, and training without presorting is often faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-\n",
    "validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "d. Use these hyperparameters to train the model on the entire training set, and then assess its\n",
    "output on the test set. You can achieve an accuracy of 85 to 87 percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d075d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "7. The steps mentioned outline the process of training and fine-tuning a Decision Tree for the moons dataset using various techniques:\n",
    "\n",
    "a) The moons dataset is generated using the `make_moons` function from Scikit-Learn, which creates a synthetic dataset with two interleaving half-moon shapes.\n",
    "\n",
    "b) The dataset is divided into a training set and a test set using the `train_test_split` function.\n",
    "\n",
    "c) Grid search with cross-validation (using `GridSearchCV`) is employed to find good hyperparameter values for the `DecisionTreeClassifier`. Different values for the `max_leaf_nodes` hyperparameter are explored.\n",
    "\n",
    "d) The model is trained on the entire training set using the selected hyperparameters and then evaluated on the test set. The accuracy achieved typically ranges from 85% to 87%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a52662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5a1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b257370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Follow these steps to grow a forest:\n",
    "\n",
    "a. Using the same method as before, create 1,000 subsets of the training set, each containing\n",
    "100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn&#39;s class.\n",
    "\n",
    "b. Using the best hyperparameter values found in the previous exercise, train one Decision\n",
    "Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision\n",
    "\n",
    "Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy,\n",
    "since they were trained on smaller sets.\n",
    "\n",
    "c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and\n",
    "keep only the most common prediction (you can do this with SciPy&#39;s mode() function). Over the test\n",
    "collection, this method gives you majority-vote predictions.\n",
    "\n",
    "d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy\n",
    "than the first model (approx 0.5 to 1.5 percent higher). You&#39;ve successfully learned a Random Forest\n",
    "classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0300ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8999b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "8. The steps mentioned describe the process of building a Random Forest classifier:\n",
    "\n",
    "a) Using the `ShuffleSplit` class from Scikit-Learn, 1,000 subsets of the training set are created, each containing 100 randomly chosen instances. This allows for the creation of different training sets for each Decision Tree.\n",
    "\n",
    "b) With the best hyperparameter values found in the previous exercise, one Decision Tree is trained on each subset of the training set. These Decision Trees are then evaluated on the test set, and their accuracy may be lower compared to the initial Decision Tree since they were trained on smaller subsets.\n",
    "\n",
    "c) For each test set case, 1,000 Decision Tree predictions are generated, and the most common prediction is selected using the `mode` function from SciPy. This results in majority-vote predictions.\n",
    "\n",
    "d) The predictions obtained from the majority voting are evaluated on the test set, and the accuracy achieved is expected to be slightly higher than that of the initial model, typically by around 0.5% to 1.5%. This demonstrates the effectiveness of the Random Forest classifier, which is an ensemble of multiple Decision Trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd10a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db5577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314daf10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a975b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7854044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af62845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfb0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646b48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a831683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a3d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8272cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f944f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
