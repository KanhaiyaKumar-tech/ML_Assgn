{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25880de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature engineering is the process of creating or selecting relevant and informative features from raw data to improve the performance of machine learning models. It involves transforming, combining, or extracting features to capture important patterns or relationships in the data. The aspects of feature engineering include:\n",
    "\n",
    "- Handling Missing Data: Addressing missing values in the data by imputation techniques or creating a separate indicator variable to capture the missingness information.\n",
    "\n",
    "- Encoding Categorical Variables: Converting categorical variables into numerical representations that can be used by machine learning algorithms. This can be done through one-hot encoding, label encoding, or target encoding.\n",
    "\n",
    "- Scaling and Normalization: Rescaling numerical features to a similar range or distribution to prevent biasing towards certain features. Common techniques include standardization (z-score normalization) or min-max scaling.\n",
    "\n",
    "- Feature Transformation: Applying mathematical or statistical transformations to the features to create new representations. Examples include logarithmic transformations, polynomial expansions, or applying mathematical functions.\n",
    "\n",
    "- Creating Interaction Features: Combining multiple features to capture interactions or higher-order relationships that may provide additional predictive power. This can involve multiplying, dividing, or adding different features.\n",
    "\n",
    "- Feature Selection: Selecting a subset of relevant features based on their importance or contribution to the model's performance. It helps reduce dimensionality, improve model interpretability, and mitigate the risk of overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfc66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e76d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection is the process of choosing a subset of relevant features from the original set of features to improve model performance, reduce dimensionality, and enhance interpretability. It aims to eliminate irrelevant or redundant features that do not contribute significantly to the learning task. The goal is to strike a balance between maintaining sufficient predictive power and avoiding the curse of dimensionality. Various methods of feature selection include:\n",
    "\n",
    "- Filter Methods: These methods rely on statistical measures or scoring metrics to evaluate the relevance of features independently of any specific learning algorithm. Examples include chi-squared test, information gain, correlation coefficient, or mutual information.\n",
    "\n",
    "- Wrapper Methods: These methods involve evaluating the performance of a specific learning algorithm using different subsets of features. They wrap the learning algorithm and use performance on validation data to guide the feature selection process. Examples include recursive feature elimination (RFE) and sequential feature selection algorithms.\n",
    "\n",
    "- Embedded Methods: These methods perform feature selection as part of the learning algorithm itself. They combine feature selection with model training and optimization, ensuring that the selected features are relevant to the specific learning algorithm. Examples include LASSO (Least Absolute Shrinkage and Selection Operator) and decision tree-based feature importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3490d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6dd23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b305ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "The feature selection filter approach evaluates the relevance of features based on statistical measures or scoring metrics independently of any specific learning algorithm. Pros of the filter approach include:\n",
    "\n",
    "- Computationally efficient, as feature evaluation is decoupled from model training.\n",
    "- Can be used as a preprocessing step before applying any learning algorithm.\n",
    "- Provides an initial assessment of feature relevance and helps in identifying potential informative features.\n",
    "\n",
    "Cons of the filter approach include:\n",
    "\n",
    "- Ignores the interactions or dependencies between features.\n",
    "- Does not consider the specific learning algorithm's performance.\n",
    "- May not result in the optimal feature subset for a specific learning task.\n",
    "\n",
    "The feature selection wrapper approach evaluates the performance of a specific learning algorithm using different subsets of features. Pros of the wrapper approach include:\n",
    "\n",
    "- Considers the performance of the learning algorithm directly, providing an indication of feature relevance for a specific task.\n",
    "- Can capture complex feature interactions and dependencies.\n",
    "- Optimizes the feature subset based on the specific learning algorithm.\n",
    "\n",
    "Cons of the wrapper approach include:\n",
    "\n",
    "- Computationally expensive, as it requires training and evaluating the model multiple times with different feature subsets.\n",
    "- May be prone to overfitting if the evaluation process is not properly controlled.\n",
    "- The selected feature subset may not generalize well to other models or tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5525f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94611e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i. The overall feature selection process typically involves the following steps:\n",
    "\n",
    "   - Data Preparation: Preprocess the data by handling missing values, encoding categorical variables, and scaling or normalizing numerical features.\n",
    "   - Feature Evaluation: Evaluate the relevance or importance of each feature using filter or wrapper methods. This can involve statistical measures, correlation analysis, or performance evaluation with different feature subsets.\n",
    "   - Feature Selection: Select a subset of relevant features based on the evaluation results. This can be done by setting a threshold on the evaluation metric, using feature importance scores, or iterative search algorithms.\n",
    "   - Model Training: Train the machine learning model using the selected feature subset and evaluate its performance on validation or test data.\n",
    "   - Iteration and Refinement: Iterate the feature selection process if needed, by adjusting thresholds or incorporating domain knowledge, to further improve the model's performance.\n",
    "\n",
    "ii. The key underlying principle of feature extraction is to transform the original features into a new set of features with reduced dimensionality while preserving the most relevant information. An example of feature extraction is Principal Component Analysis (PCA), where the original features are linearly transformed into a new set of orthogonal features called principal components. The most widely used feature extraction algorithms include PCA, Linear Discriminant Analysis (LDA), and Non-negative Matrix Factorization (NMF).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce13ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3243abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Describe the feature engineering process in the sense of a text categorization issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "In text categorization, the feature engineering process involves converting raw text data into meaningful numerical representations that can be used by machine learning algorithms. This typically includes steps such as:\n",
    "\n",
    "- Text Preprocessing: Removing stop words, punctuation, and special characters. Tokenizing the text into individual words or n-grams.\n",
    "- Feature Extraction: Creating a document-term matrix or using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) to represent the frequency or importance of words or n-grams in each document.\n",
    "- Feature Selection: Selecting relevant words or n-grams based on their frequency, statistical measures, or information gain.\n",
    "- Encoding: Transforming the selected features into numerical representations, such as one-hot encoding or term frequency.\n",
    "- Scaling: Scaling the numerical features to a similar range or applying normalization techniques.\n",
    "- Model Training: Training a machine learning model using the engineered features and evaluating its performance on validation or test data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a4988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745af044",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine similarity is a good metric for text categorization because it measures the similarity between two vectors in a high-dimensional space, such as the space of word frequencies. It is particularly suitable for comparing documents or text samples based on their content. Cosine similarity is robust to differences in document length and captures the angle between the vectors, indicating the degree of similarity or dissimilarity. To calculate the cosine similarity, the dot product of the two vectors is divided by the product of their magnitudes.\n",
    "\n",
    "For the given document-term matrix rows: (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1), the cosine similarity can be calculated as follows:\n",
    "\n",
    "Cosine Similarity = (2 * 2 + 3 * 1 + 2 * 0 + 0 * 0 + 2 * 3 + 3 * 2 + 3 * 1 + 0 * 3 + 1 * 1) / sqrt((2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) * (2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2))\n",
    "\n",
    "The resemblance in cosine similarity is the calculated value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72494910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "i. The formula for calculating the Hamming distance is the number of positions at which two binary strings of equal length differ. In the given example, between 10001011 and 11001111, the Hamming distance can be calculated as follows:\n",
    "\n",
    "Hamming Distance = 4 (positions where the bits differ)\n",
    "\n",
    "ii. The Jaccard index is calculated as the size of the intersection divided by the size of the union of two sets. The similarity matching coefficient is calculated as the number of elements in the intersection divided by the number of elements in the smallest set. For the two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 0, 0, 1, 1, 0, 0, 1), the Jaccard index can be calculated as follows:\n",
    "\n",
    "Jaccard Index = 3 (intersection) / 6 (union) = 0.5\n",
    "\n",
    "The similarity matching coefficient can be calculated as follows:\n",
    "\n",
    "Similarity Matching Coefficient = 3 (intersection) / 4 (smallest set) = 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114dfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f02b90",
   "metadata": {},
   "outputs": [],
   "source": [
    " In a high-dimensional dataset, the number of features or dimensions is significantly larger compared to the number of instances. Real-life examples of high-dimensional datasets include text documents represented by word frequencies, gene expression datasets, or image datasets with pixel values. Difficulties in using machine learning techniques on high-dimensional datasets include:\n",
    "\n",
    "- Curse of Dimensionality: The increased number of features leads to sparsity and makes it challenging to find meaningful patterns or relationships within the data.\n",
    "- Increased Computational Complexity: Training and evaluating models on high-dimensional data can be computationally expensive and time-consuming.\n",
    "- Overfitting: With a high number of dimensions, models are more prone to overfitting, as they can easily memorize noise or specific patterns present in the training data.\n",
    "- Interpretability: Interpreting the impact or importance of individual features becomes more challenging in high-dimensional spaces.\n",
    "\n",
    "To address these difficulties, dimensionality reduction techniques such as Principal Component Analysis (PCA), feature selection, or regularization methods can be applied to reduce the number of dimensions and focus on the most informative features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14896bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Make a few quick notes on:\n",
    "\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5664a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quick notes:\n",
    "\n",
    "- PCA stands for Principal Component Analysis, which is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while retaining the most significant variations in the data.\n",
    "- Vectors are mathematical representations of data points or features in a multidimensional space. They can capture the magnitude and direction of quantities or attributes.\n",
    "- Embedded technique refers to incorporating feature selection as part of the learning algorithm itself. It combines feature selection and model training, ensuring that the selected features are relevant to the specific learning algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aea203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf50b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparison:\n",
    "\n",
    "- Sequential backward exclusion vs. sequential forward selection: \n",
    "  - Sequential backward exclusion starts with all features and iteratively removes one feature at a time based on a specific criterion (e.g., performance degradation). It aims to find the optimal subset of features by progressively eliminating irrelevant or redundant features.\n",
    "  - Sequential forward selection starts with an empty set and iteratively adds one feature at a time based on a specific criterion (e.g., performance improvement). It aims to find the optimal subset of features by progressively adding the most relevant or informative features.\n",
    "\n",
    "- Filter vs. wrapper methods of feature selection:\n",
    "  - Filter methods evaluate feature relevance independently of any specific learning algorithm, using statistical measures or scoring metrics. They are computationally efficient but do not consider the performance of the learning algorithm directly.\n",
    "  - Wrapper methods evaluate feature relevance based on the performance of a specific learning algorithm using different subsets of features. They consider the specific learning algorithm's performance but can be computationally expensive.\n",
    "\n",
    "- SMC (Similarity Matching Coefficient) vs. Jaccard coefficient:\n",
    "  - SMC calculates the number of common elements divided by the number of elements in the smallest set. It measures the similarity or overlap between two sets of binary features.\n",
    "  - Jaccard coefficient calculates the size of the intersection divided by the size of the union of two sets. It measures the similarity or overlap between two sets, taking into account both common and non-common elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e76b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b50e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888b11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5763c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02e091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84351ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408ed25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf03829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb258b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74841b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5dbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef03c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1749372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f2a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ecacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79a66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4b4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43238b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4867c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
