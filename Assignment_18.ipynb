{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f0310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the difference between supervised and unsupervised learning? Give some examples to\n",
    "illustrate your point.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca784ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    " The main difference between supervised and unsupervised learning lies in the availability of labeled data. In supervised learning, the training data includes both input variables (features) and their corresponding output variables (labels), and the goal is to learn a mapping function from the input to the output. Examples of supervised learning include image classification, spam detection, and sentiment analysis. On the other hand, unsupervised learning deals with unlabeled data, where the goal is to discover hidden patterns, structures, or relationships in the data. Examples of unsupervised learning include clustering, anomaly detection, and dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bfe4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Mention a few unsupervised learning applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    " Unsupervised learning has various applications, including:\n",
    "   - Clustering: Grouping similar items together, such as customer segmentation based on purchasing behavior.\n",
    "   - Anomaly detection: Identifying rare or unusual instances, such as fraud detection in financial transactions.\n",
    "   - Dimensionality reduction: Reducing the number of features while preserving important information, such as in image or text data.\n",
    "   - Association mining: Discovering interesting relationships or patterns in large datasets, such as market basket analysis to identify frequently co-occurring items.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad381e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What are the three main types of clustering methods? Briefly describe the characteristics of each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21feb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    " The three main types of clustering methods are:\n",
    "   - Partition-based clustering: Divides the data into distinct non-overlapping clusters. Examples include k-means and k-medoids algorithms.\n",
    "   - Hierarchical clustering: Builds a hierarchy of clusters by iteratively merging or splitting them based on certain criteria. It can be agglomerative (bottom-up) or divisive (top-down).\n",
    "   - Density-based clustering: Identifies clusters based on the density of data points in the feature space. Examples include DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and OPTICS (Ordering Points To Identify the Clustering Structure).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain how the k-means algorithm determines the consistency of clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84afcad",
   "metadata": {},
   "outputs": [],
   "source": [
    " The k-means algorithm determines the consistency of clustering by minimizing the within-cluster sum of squared errors (SSE). It achieves this by iteratively assigning data points to the nearest cluster centroid and updating the centroid based on the mean of the assigned points. The algorithm continues until convergence, where the assignments and centroids no longer change significantly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02304c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. With a simple illustration, explain the key difference between the k-means and k-medoids\n",
    "algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba85c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    " The key difference between the k-means and k-medoids algorithms is the way cluster representatives or centroids are chosen. In k-means, the centroid is the mean (average) of all data points assigned to the cluster, while in k-medoids, the centroid is an actual data point from the cluster that minimizes the total dissimilarity to other points in the cluster. K-medoids is more robust to outliers compared to k-means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41051f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is a dendrogram, and how does it work? Explain how to do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe917ca",
   "metadata": {},
   "outputs": [],
   "source": [
    " A dendrogram is a tree-like diagram that represents the hierarchical relationships between objects or clusters. It is commonly used in hierarchical clustering. The dendrogram shows how clusters are merged or divided at each level of the hierarchy. The vertical axis represents the dissimilarity or similarity between objects or clusters, and the horizontal axis represents the objects or clusters themselves. By analyzing the dendrogram, one can determine the optimal number of clusters to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f45bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What exactly is SSE? What role does it play in the k-means algorithm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1bed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE (Sum of Squared Errors) is a measure of the compactness or tightness of clusters in the k-means algorithm. It quantifies the within-cluster variation by summing the squared distances between each data point and its assigned cluster centroid. The goal of k-means is to minimize SSE by finding the optimal cluster centroids that minimize the total squared distance within each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8709df",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. With a step-by-step algorithm, explain the k-means procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa117eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    " The k-means algorithm follows these steps:\n",
    "   1. Initialize the cluster centroids randomly or based on some heuristic.\n",
    "   2. Assign each data point to the nearest centroid (cluster) based on Euclidean distance.\n",
    "   3. Update the centroids by calculating the mean of all data points assigned to each cluster.\n",
    "   4. Repeat steps 2 and 3 until convergence (when the assignments and centroids no longer change significantly) or a maximum number of iterations is reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6dae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "9. In the sense of hierarchical clustering, define the terms single link and complete link.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7408f",
   "metadata": {},
   "outputs": [],
   "source": [
    " In hierarchical clustering, the terms single link and complete link refer to different distance measures used to determine the similarity between clusters:\n",
    "   - Single link: The similarity between two clusters is based on the minimum distance between any pair of points, one from each cluster. It tends to form long, elongated clusters.\n",
    "   - Complete link: The similarity between two clusters is based on the maximum distance between any pair of points, one from each cluster. It tends to form compact, spherical clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. How does the apriori concept aid in the reduction of measurement overhead in a business\n",
    "basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92415e01",
   "metadata": {},
   "outputs": [],
   "source": [
    " The apriori concept aids in reducing measurement overhead in business basket analysis by leveraging the idea of frequent itemsets. In this concept, frequent itemsets are identified based on a minimum support threshold. By focusing only on frequent itemsets, which are items or item combinations that occur above the minimum support threshold, the analysis can concentrate on relevant patterns and reduce the computational burden. For example, in a retail store, the apriori algorithm can identify frequently co-purchased items, such as customers who buy bread also tend to buy butter, without considering all possible item combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807cacb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad478378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344fa86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312dfc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ab4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2818481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e15ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674efabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889b77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859cbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022cfd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ab4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
