{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the underlying concept of Support Vector Machines?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f47c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. The underlying concept of Support Vector Machines (SVMs) is to find an optimal hyperplane that separates data points belonging to different classes in a given dataset. SVMs aim to maximize the margin between the hyperplane and the nearest data points from each class, called support vectors. This approach helps to create a decision boundary that generalizes well to unseen data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc26e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is the concept of a support vector?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. A support vector in SVM refers to the data points that are closest to the decision boundary (hyperplane) and have the most influence in defining the boundary. These support vectors play a crucial role in determining the optimal hyperplane and maximizing the margin between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. When using SVMs, why is it necessary to scale the inputs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Scaling the inputs is necessary when using SVMs because SVM algorithms are sensitive to the scale of features. If the features have different scales, it can lead to bias in the optimization process, as SVM tries to minimize the objective function based on distances. Scaling the inputs ensures that all features contribute equally and prevents features with larger scales from dominating the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f90d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a\n",
    "percentage chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. An SVM classifier can output a confidence score, typically in the form of the signed distance from the decision boundary to the input data point. However, this confidence score does not directly represent a percentage chance or probability. SVMs are not inherently probabilistic models, and interpreting the confidence score as a probability requires additional calibration, such as Platt scaling or using alternative techniques like support vector probability machines (SVM-Prob).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9504e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "5. Should you train a model on a training set with millions of instances and hundreds of features\n",
    "using the primal or dual form of the SVM problem?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "5. When training a model on a large training set with millions of instances and hundreds of features, it is generally more efficient to use the dual form of the SVM problem. The primal form of the problem involves solving for the weights directly, while the dual form involves solving for Lagrange multipliers associated with support vectors. The dual form allows for more efficient computations when the number of features is large compared to the number of instances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the\n",
    "training collection. Is it better to raise or lower (gamma)? What about the letter C?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. If an SVM classifier with an RBF kernel underfits the training collection, increasing the value of the gamma parameter can be beneficial. A higher gamma value makes the kernel function more sensitive to individual data points, which can help capture complex patterns. Similarly, decreasing the value of the C parameter can also be beneficial, as it allows for more flexibility in the margin and can help reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c127c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should\n",
    "the QP parameters (H, f, A, and b) be set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf quadratic programming (QP) solver, the QP parameters (H, f, A, and b) should be set as follows:\n",
    "- H is the Hessian matrix, which is a positive semi-definite matrix based on the kernel function and training data.\n",
    "- f is the vector of linear coefficients that represents the bias term and constraints of the optimization problem.\n",
    "- A is the matrix representing the equality constraints (usually set to the transpose of the labels).\n",
    "- b is the vector representing the inequality constraints (usually set to -1 for the negative class and +1 for the positive class).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de023c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and\n",
    "an SGDClassifier. See if you can get them to make a model that is similar to yours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75842950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "8. Training a LinearSVC, SVC, and SGDClassifier on the same linearly separable dataset should yield similar models in terms of the decision boundary. LinearSVC and SVC with a linear kernel essentially solve the same problem using different optimization algorithms (Liblinear and Libsvm, respectively). SGDClassifier with a linear loss function and hinge loss is also similar but uses stochastic gradient descent for optimization. However, slight differences in model parameters or convergence criteria might lead to slightly different results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa39b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all\n",
    "10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want\n",
    "to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Training an SVM classifier on the MNIST dataset using one-versus-the-rest strategy can achieve a high level of precision for digit recognition. By tuning the hyperparameters, such as the regularization parameter (C) and the kernel function, and employing cross-validation on small validation sets, it is possible to achieve precision rates exceeding 95% or even higher, depending on the specific settings and feature representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b00a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a773aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Training an SVM regressor on the California housing dataset involves predicting the continuous target variable, such as house prices, based on the input features. SVM regressors use a loss function to optimize the model's fit to the data. The choice of kernel function, regularization parameter (C), and other hyperparameters can influence the model's performance. By tuning these parameters and employing appropriate feature engineering techniques, an SVM regressor can provide accurate predictions of housing prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ab154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40792976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a515d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ed434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57255d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdefd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b4645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3e16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65578842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489bccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd629e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a434609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f24747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fff3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c350c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b7ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f883218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795500e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a7515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914de339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d4315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
